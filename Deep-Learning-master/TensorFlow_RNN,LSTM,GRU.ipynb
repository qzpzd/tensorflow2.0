{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用TensorFlow实现RNN/LSTM/GRU\n",
    "参考自https://blog.csdn.net/jmh1996/article/details/78821216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784) (55000, 10)\n",
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.images.shape,mnist.train.labels.shape)\n",
    "print(mnist.test.images.shape,mnist.test.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_rate=0.001  #学习速率\n",
    "train_step=1000  \n",
    "batch_size=1280    #每批样本数\n",
    "display_step=100   #控制输出频次\n",
    "\n",
    "frame_size=28     #序列里面每一个分量的大小。因为每个分量都是一行像素，而一行像素有28个像素点。所以frame_size为28\n",
    "sequence_length=28  #每个样本序列的长度。因为我们希望把一个28x28的图片当做一个序列输入到rnn进行训练，所以我们需要对图片进行序列化。一种最方便的方法就是我们认为行与行之间存在某些关系，于是把图片的每一行取出来当做序列的一个维度。所以这里sequence_size就是设置为28。\n",
    "hidden_num=100  #隐层个数\n",
    "n_classes=10  #类别数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN/LSTM/GRU模型\n",
    "单层RNN/LSTM/GRU：只需在第12行中修改为BasicRNNCell/BasicLSTMCell/GRUCell即可，其余的都不变\n",
    "\n",
    "多层RNN/LSTM/GRU：将第12行变成第13行（即tf.nn.rnn_cell.MultiRNNCell这行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step accuracy loss\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "100 0.777344 1.68638\n",
      "200 0.876562 1.58436\n",
      "300 0.971094 1.49195\n",
      "400 0.971875 1.48832\n",
      "500 0.983594 1.47964\n",
      "600 0.976563 1.4832\n",
      "700 0.983594 1.47919\n",
      "800 0.9875 1.4749\n",
      "900 0.985156 1.47906\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    #定义输入,输出\n",
    "    x=tf.placeholder(dtype=tf.float32,shape=[None,sequence_length*frame_size],name=\"inputx\")\n",
    "    y=tf.placeholder(dtype=tf.float32,shape=[None,n_classes],name=\"expected_y\")\n",
    "    #定义权值\n",
    "    weights=tf.Variable(tf.truncated_normal(shape=[hidden_num,n_classes]))\n",
    "    bias=tf.Variable(tf.zeros(shape=[n_classes]))\n",
    "\n",
    "    # 定义RNN网络\n",
    "    def RNN(x,weights,bias):\n",
    "        '''返回[batch_size,n_classes]'''\n",
    "        x=tf.reshape(x,shape=[-1,sequence_length,frame_size])\n",
    "#         rnn_cell=tf.nn.rnn_cell.BasicRNNCell(hidden_num) # RNN/LSTM/GRU在此处选择BasicRNNCell/BasicLSTMCell/GRUCell。该网络中包含一个深度RNN网络，这个RNN包含hidden_num个隐层单元/RNN cell\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.GRUCell(hidden_num) for _ in range(3)]) # 构建多层RNN/LSTM/GRU网络,3表示3层（这里都是用MultiRNNCell，没有MultiGRUCell等）\n",
    "    \n",
    "        output,states=tf.nn.dynamic_rnn(rnn_cell,x,dtype=tf.float32)\n",
    "        return tf.nn.softmax(tf.matmul(output[:,-1,:],weights)+bias,1)\n",
    "\n",
    "    # 计算预计输出\n",
    "    predy=RNN(x,weights,bias)\n",
    "    # 定义损失函数和优化算法\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predy,labels=y))\n",
    "    train=tf.train.AdamOptimizer(train_rate).minimize(cost)\n",
    "    # 计算accuracy\n",
    "    correct_pred=tf.equal(tf.argmax(predy,1),tf.argmax(y,1))\n",
    "    accuracy=tf.reduce_mean(tf.to_float(correct_pred))\n",
    "\n",
    "\n",
    "## 开始训练\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    print('step','accuracy','loss')\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    step=1\n",
    "    testx,testy=mnist.test.next_batch(batch_size)\n",
    "    while step<train_step:\n",
    "        batch_x,batch_y=mnist.train.next_batch(batch_size)\n",
    "    #    batch_x=tf.reshape(batch_x,shape=[batch_size,sequence_length,frame_size])\n",
    "        _loss,__=sess.run([cost,train],feed_dict={x:batch_x,y:batch_y})\n",
    "        if step % display_step ==0:\n",
    "            acc,loss=sess.run([accuracy,cost],feed_dict={x:testx,y:testy})\n",
    "            print(step,acc,loss)\n",
    "\n",
    "        step+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
